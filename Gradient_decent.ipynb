{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8670915509586015,\n",
       " 1.0821419388996176,\n",
       " [1.40625,\n",
       "  0.6096515624999999,\n",
       "  0.2654613632812499,\n",
       "  0.11672851939453116,\n",
       "  0.05244084169123535,\n",
       "  0.024636996411858163,\n",
       "  0.012595926308505026,\n",
       "  0.007365370338729636,\n",
       "  0.005077618104763821,\n",
       "  0.004061671208223951])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementing gradient descent for linear regression using the example provided\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Given dataset: [(x1 = 1, y1 = 2), (x2 = 2, y2 = 3)]\n",
    "X = np.array([1, 2])  # Features (input data)\n",
    "y = np.array([2, 3])  # Target values (output data)\n",
    "m = len(y)  # Number of training examples\n",
    "\n",
    "# Initial values\n",
    "theta_0 = 0  # Initial guess for intercept\n",
    "theta_1 = 0  # Initial guess for slope\n",
    "alpha = 0.1  # Learning rate\n",
    "iterations = 100  # Number of iterations for gradient descent\n",
    "\n",
    "# Cost function history for plotting\n",
    "cost_history = []\n",
    "\n",
    "# Function to compute the cost (Mean Squared Error)\n",
    "def compute_cost(X, y, theta_0, theta_1):\n",
    "    predictions = theta_0 + theta_1 * X\n",
    "    return (1/(2 * m)) * np.sum((predictions - y) ** 2)\n",
    "\n",
    "# Gradient Descent Algorithm\n",
    "def gradient_descent(X, y, theta_0, theta_1, alpha, iterations):\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Predictions of the linear model\n",
    "        predictions = theta_0 + theta_1 * X\n",
    "        \n",
    "        # Compute the gradients\n",
    "        d_theta_0 = (1/m) * np.sum(predictions - y)\n",
    "        d_theta_1 = (1/m) * np.sum((predictions - y) * X)\n",
    "        \n",
    "        # Update the parameters\n",
    "        theta_0 = theta_0 - alpha * d_theta_0\n",
    "        theta_1 = theta_1 - alpha * d_theta_1\n",
    "        \n",
    "        # Compute and store the cost after each iteration\n",
    "        cost = compute_cost(X, y, theta_0, theta_1)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "    return theta_0, theta_1, cost_history\n",
    "\n",
    "# Running gradient descent\n",
    "theta_0, theta_1, cost_history = gradient_descent(X, y, theta_0, theta_1, alpha, iterations)\n",
    "\n",
    "# Final parameters after gradient descent\n",
    "theta_0, theta_1, cost_history[:10]  # Return first 10 cost values to check convergence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 \n",
      "X value is 1.4\n",
      "Iteration 2 \n",
      "X value is 0.11999999999999966\n",
      "Iteration 3 \n",
      "X value is -0.9040000000000001\n",
      "Iteration 4 \n",
      "X value is -1.7232000000000003\n",
      "Iteration 5 \n",
      "X value is -2.3785600000000002\n",
      "Iteration 6 \n",
      "X value is -2.902848\n",
      "Iteration 7 \n",
      "X value is -3.3222784\n"
     ]
    }
   ],
   "source": [
    "cur_x = 3 # The algorithm starts at x=3\n",
    "\n",
    "rate = 0.1 # Learning rate\n",
    "\n",
    "precision = 0.5 #this tells us when to stop the algorithm\n",
    "\n",
    "previous_step_size = 1 #\n",
    "\n",
    "max_iters = 10000 # maximum number of iterations\n",
    "\n",
    "iters = 0\n",
    "\n",
    "df = lambda x: 2*(x+5) #Gradient of our function\n",
    "\n",
    "while previous_step_size > precision and iters < max_iters:\n",
    "\n",
    "    prev_x = cur_x #Store current x value in prev_x\n",
    "\n",
    "    cur_x = cur_x - rate * df(prev_x) #Grad descent\n",
    "\n",
    "    previous_step_size = abs(cur_x - prev_x) #Change in x\n",
    "\n",
    "    iters = iters+1 #iteration count\n",
    "\n",
    "    print(\"The local minimum occurs at\",iters,\"\\nX value is\",cur_x) #Print iterations \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
